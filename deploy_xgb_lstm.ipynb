{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf5ea8a",
   "metadata": {},
   "source": [
    "# Steps to Use the Model in a Real-World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17b09e",
   "metadata": {},
   "source": [
    "## 1. Convert the Model into a Predictive API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cef851",
   "metadata": {},
   "source": [
    "### Deploying the model as an API will allow real-time or batch predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e912c97",
   "metadata": {},
   "source": [
    "#### Options:\n",
    "- **FastAPI** (recommended for speed & scalability)\n",
    "- **Flask** (simpler but slower)\n",
    "- **Django REST Framework** (if using Django-based apps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fe209",
   "metadata": {},
   "source": [
    "## 2. Save and Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save XGBoost model\n",
    "joblib.dump(xgb_model, \"xgb_model.pkl\")\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model.save(\"lstm_model.h5\")\n",
    "\n",
    "# Load models later:\n",
    "xgb_model = joblib.load(\"xgb_model.pkl\")\n",
    "lstm_model = tf.keras.models.load_model(\"lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db9a83",
   "metadata": {},
   "source": [
    "## 3. Create a FastAPI Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Load models\n",
    "xgb_model = joblib.load(\"xgb_model.pkl\")\n",
    "lstm_model = tf.keras.models.load_model(\"lstm_model.h5\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define request schema\n",
    "class PowerInput(BaseModel):\n",
    "    features: list  # List of numerical feature values\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict_power(input_data: PowerInput):\n",
    "    X_input = np.array(input_data.features).reshape(1, -1)\n",
    "    \n",
    "    # XGBoost prediction\n",
    "    xgb_pred = xgb_model.predict(X_input)\n",
    "    \n",
    "    # Prepare input for LSTM (reshape to 3D)\n",
    "    X_input_lstm = np.reshape(X_input, (1, 1, X_input.shape[1]))\n",
    "    lstm_pred = lstm_model.predict(X_input_lstm).flatten()\n",
    "    \n",
    "    # Final power consumption prediction\n",
    "    final_pred = xgb_pred + lstm_pred\n",
    "    return {\"predicted_power_consumption\": final_pred.tolist()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b413b0d",
   "metadata": {},
   "source": [
    "### Run API:\n",
    "```bash\n",
    "uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fafe2",
   "metadata": {},
   "source": [
    "## 4. Deploy to the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da60ea6",
   "metadata": {},
   "source": [
    "#### Deployment Options:\n",
    "- **AWS Lambda + API Gateway** (serverless)\n",
    "- **Google Cloud Run** (auto-scaling)\n",
    "- **Docker + Kubernetes** (production-ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447a652",
   "metadata": {},
   "source": [
    "### Example Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FROM python:3.9\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY . .\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c8b1f",
   "metadata": {},
   "source": [
    "### Build and Run Docker:\n",
    "```bash\n",
    "docker build -t power-prediction .\n",
    "docker run -p 8000:8000 power-prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04d326",
   "metadata": {},
   "source": [
    "## 5. Create a Dashboard (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "st.title(\"Power Consumption Prediction\")\n",
    "\n",
    "features = st.text_input(\"Enter feature values (comma-separated)\")\n",
    "if st.button(\"Predict\"):\n",
    "    data = {\"features\": list(map(float, features.split(\",\")))}\n",
    "    response = requests.post(\"http://localhost:8000/predict\", json=data)\n",
    "    st.write(\"Predicted Power Consumption:\", response.json()[\"predicted_power_consumption\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119ffc4",
   "metadata": {},
   "source": [
    "### Run Streamlit:\n",
    "```bash\n",
    "streamlit run dashboard.py\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}